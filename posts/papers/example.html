<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Notes</title>
  <link rel="stylesheet" href="../../css/style.css">
</head>

<body>

<header>
  <h1 class="site-title"><a href="../../index.html">Research Notes</a></h1>
  <nav>
    <a href="../../papers.html">Papers</a>
    <a href="../../notes.html">Notes</a>
    <a href="../../diary.html">Diary</a>
  </nav>
</header>

<main>

  <article class="paper-review">

    <h2 class="paper-title">
      Vision Transformers Revisited
    </h2>

    <p class="paper-meta">
      <strong>Authors:</strong> Doe et al. <br>
      <strong>Conference:</strong> ICLR 2023 <br>
      <strong>Link:</strong>
      <a href="https://arxiv.org/abs/xxxx.xxxxx" target="_blank">arXiv</a>
    </p>

    <section>
      <h3>Why I read this paper</h3>
      <p>
        최근 Vision Transformer에서 attention sparsity가 성능에 미치는
        영향을 분석하고 싶어서 읽었다.
      </p>
    </section>

    <section>
      <h3>Main idea</h3>
      <ul>
        <li>Dense attention performs better for vision tasks</li>
        <li>Sparse attention hurts spatial reasoning</li>
      </ul>
    </section>

    <section>
      <h3>Method</h3>
      <p>
        다양한 attention sparsity 수준을 조절하며 ImageNet에서 성능을 비교하였다.
      </p>
    </section>

    <section>
      <h3>My thoughts</h3>
      <p>
        이 결과는 내가 진행 중인 ViT attention 연구와 직접적으로 연결된다.
        특히 small patch regime에서 의미가 크다.
      </p>
    </section>

  </article>

</main>

</body>

</html>